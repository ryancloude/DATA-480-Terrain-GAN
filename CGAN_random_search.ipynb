{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmt_iC3qKcdj"
   },
   "outputs": [],
   "source": [
    "path_for_joined_geotiff_df = '' \n",
    "path_for_folder_to_save_models = ''\n",
    "path_for_tf_dataset = ''\n",
    "\n",
    "#Number of models to be trained\n",
    "search_iterations = 100\n",
    "#Max Number of epochs to train each model. Most models will probably fail before reaching max number of epochs.\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajdaWFE9cEjs",
    "outputId": "18cc8418-b98b-4218-983d-7f80850301f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from google.colab import drive\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "from skimage.transform import resize\n",
    "#drive.mount('/content/drive')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvIJjN9K-kUo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_for_joined_geotiff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeIV-lo1WhZX"
   },
   "outputs": [],
   "source": [
    "#These Rows (from yoesmetie) had random points with elevation in the billions, so I just exluded them\n",
    "outliers = [964, 970, 975, 976, 977, 982, 983, 984, 985, 992, 993, 994, 995]\n",
    "df = df[~df.index.isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2joAuWp5veXH"
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ou0X5TxsXGP"
   },
   "outputs": [],
   "source": [
    "#Loads Tensorflow dataset tf_dataset_128x128\n",
    "dataset = tf.data.experimental.load(path_for_tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxf8aYrFdTaw"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(len(dataset)/batch_size))\n",
    "num_classes = len(np.unique(df.BIOME_NAME))\n",
    "latent_dim = ((16*16)*num_classes)-num_classes\n",
    "generator_input = latent_dim + num_classes\n",
    "discriminator_in_channels = num_channels + num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgfaqFXHIJiw"
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    discriminator = keras.Sequential(\n",
    "      [\n",
    "          keras.layers.InputLayer((image_size, image_size, discriminator_in_channels)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\" \n",
    "                        , activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\" \n",
    "                        , activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.Conv2D(256, (4, 4), strides=(2, 2), padding=\"same\" \n",
    "                        , activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.Conv2D(512, (4, 4), strides=(2, 2), padding=\"same\" ,\n",
    "                        activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.Conv2D(1024, (3, 3), strides=(2, 2), padding=\"same\", \n",
    "                         activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.GlobalMaxPooling2D(),\n",
    "          layers.Dense(1, activation='sigmoid')\n",
    "      ],\n",
    "      name=\"discriminator\",\n",
    "  )\n",
    "  return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQmPkPlhISfz"
   },
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        #load data\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dimension for the labels\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        )\n",
    "\n",
    "        # Generate random points in the latent space.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        #Add the labels\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "        #Generate fake terrain with generator\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        #Combine terrain with labels\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        #Real and Fake Terrain\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "        \n",
    "        #Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        #Track loss\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "\n",
    "        return{\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDEb91DZgGSR"
   },
   "outputs": [],
   "source": [
    "#If true then generator's first layer is a dense layer\n",
    "dense = [True, False]\n",
    "#Number of conv2d layers in generator\n",
    "conv2d_layers = [i for i in range(3,8)]\n",
    "#Alpha used in generator\n",
    "alpha = [.01, .1, .2, .4, .8]\n",
    "#l2 used in generator\n",
    "l2 = [1*10**-i for i in range(1,7)]\n",
    "#first filter size\n",
    "filter_size = [2**i for i in range(11,1,-1)]\n",
    "#Determines how filter size will change from the first filter size\n",
    "filter_growth = ['decreasing', 'constant', 'increasing']\n",
    "#first kernel size\n",
    "kernel_size = [2**i for i in range(2,7)]\n",
    "##Determines how kernel size will change from the first kernel size\n",
    "kernel_growth = ['decreasing', 'constant', 'increasing']\n",
    "#possible momemtum in generator\n",
    "momentum = [.8, .5, .3, .1, .05]\n",
    "#generator's possible learning ratae\n",
    "learning_rate = [.1, .05, .01, .005, .001, .0005, .0001, .00005, .00001]\n",
    "#possible steps per learning rate decay\n",
    "steps_per_lr_decay = [1, steps_per_epoch//5, steps_per_epoch//2, steps_per_epoch, steps_per_epoch*2, steps_per_epoch*5]\n",
    "#Dictionary of the all the lists of possible params\n",
    "possible_params = {'dense': dense, 'conv2d_layers': conv2d_layers, 'alpha': alpha, \n",
    "                     'l2':l2, 'filter_size': filter_size, 'filter_growth': filter_growth,'kernel_size':kernel_size, 'kernel_growth':kernel_growth, \n",
    "                     'momentum':momentum, 'learning_rate': learning_rate, 'steps_per_lr_decay':steps_per_lr_decay}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXN5wekza1k1"
   },
   "outputs": [],
   "source": [
    "#Randomly chooses params from param space adn returns dictionary of choosen params\n",
    "def get_params(param_space):\n",
    "    choosen_params = {}\n",
    "    #loops through each param\n",
    "    for param_name, param in param_space.items():\n",
    "        #chooses random value from list of possible value for each parameter\n",
    "        choosen_params[param_name] = random.choice(param)\n",
    "    return choosen_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PoCavfVztcVP"
   },
   "outputs": [],
   "source": [
    "#Returns list of kernel and filters sizes for each conv2d_layer\n",
    "def get_kernels_and_filters(first_filter_size, filter_growth, first_kernel_size, kernel_growth, conv2d_layers):\n",
    "  #Used to constrain the size of the model when increasing and decreasing growth for kernel and filter size\n",
    "  max_filter = 2048\n",
    "  min_filter = 4\n",
    "  max_kernel = 32\n",
    "  min_kernel = 4\n",
    "  if filter_growth == 'constant':\n",
    "    #All filter sizes will be the same size as the first filter\n",
    "    filters = [first_filter_size for i in range(conv2d_layers)]\n",
    "  elif filter_growth == 'increasing':\n",
    "    #filter sizes will be first_filter_size*(2^i) for i=0 to i = conv2d_layers\n",
    "    filters = [first_filter_size*2**i for i in range(conv2d_layers)]\n",
    "  else:\n",
    "    ##filter sizes will be first_filter_size*(2^-i) for i=0 to i = conv2d_layers\n",
    "    filters = [first_filter_size*2**-i for i in range(conv2d_layers)]  \n",
    "  if kernel_growth == 'constant':\n",
    "    #All kernel sizes will be the same size as the first kernel\n",
    "    kernels = [first_kernel_size for i in range(conv2d_layers)]\n",
    "  elif kernel_growth == 'increasing':\n",
    "    #kernel sizes will be first_kernel_size*(2^i) for i=0 to i = conv2d_layers\n",
    "    kernels = [first_kernel_size*2**i for i in range(conv2d_layers)]\n",
    "  else:\n",
    "    #kernel sizes will be first_kernel_size*(2^-i) for i=0 to i = conv2d_layers\n",
    "    kernels = [first_kernel_size*2**-i for i in range(conv2d_layers)]\n",
    "  kernels =  np.asarray(kernels)\n",
    "  filters = np.asarray(filters)\n",
    "  #if a filter/kernel size is greater/less than max/min than it is set to the max/min\n",
    "  kernels[kernels < min_kernel] = min_kernel\n",
    "  kernels[kernels > max_kernel] = max_kernel\n",
    "  filters[filters > max_filter] = max_filter\n",
    "  filters[filters < min_filter] = min_filter\n",
    "  return kernels, filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkM28DQlxf8j"
   },
   "outputs": [],
   "source": [
    "#returns list of strides for each conv2d layer\n",
    "def get_strides(conv2d_layers):\n",
    "  #Every models last two conv2d layers will have (2,2) strides\n",
    "  strides = [(2,2), (2,2)]\n",
    "  for i in range(conv2d_layers-3):\n",
    "    strides.insert(0,(1,1))\n",
    "  return strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye2CfRcWQqy3"
   },
   "outputs": [],
   "source": [
    "#Stops training if discriminator loss or generator loss gets close to zero\n",
    "class Custom_Callback(keras.callbacks.Callback):\n",
    "   def on_epoch_end(self, epoch, logs=None): \n",
    "         if (logs.get('d_loss') < .0001) or (logs.get('g_loss') < .0001):\n",
    "           self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOB0Ua3nhcQY"
   },
   "outputs": [],
   "source": [
    "#Builds generator with the random params\n",
    "def build_generator(iter, choosen_params):\n",
    "  strides = get_strides(choosen_params['conv2d_layers'])\n",
    "  kernels, filters = get_kernels_and_filters(first_filter_size=choosen_params['filter_size'], filter_growth=choosen_params['filter_growth'], \n",
    "                          first_kernel_size=choosen_params['kernel_size'], kernel_growth=choosen_params['kernel_growth'], conv2d_layers=choosen_params['conv2d_layers'])\n",
    "  generator = keras.Sequential(name=\"generator\" + str(iter))\n",
    "  generator.add(keras.layers.InputLayer(generator_input,))\n",
    "  #Maps latent space to 2d space using dense layers\n",
    "  if choosen_params['dense']:\n",
    "    generator.add(layers.Dense(16*16*num_classes, activity_regularizer=keras.regularizers.L2(choosen_params['l2'])))\n",
    "    generator.add(layers.LeakyReLU(alpha=choosen_params['alpha']))\n",
    "    generator.add(layers.BatchNormalization(momentum=choosen_params['momentum']))\n",
    "    generator.add(layers.Reshape((16, 16, num_classes)))\n",
    "  else:\n",
    "    #If dense layer isn't used then latent space is reshaped to 2d\n",
    "    generator.add(layers.Reshape((16, 16, num_classes)))\n",
    "  for i in range(choosen_params['conv2d_layers']-1):\n",
    "    generator.add(layers.Conv2DTranspose(filters[i], (int(kernels[i]), int(kernels[i])),\n",
    "                                           padding='same', activity_regularizer=keras.regularizers.L2(choosen_params['l2']), strides=strides[i]))\n",
    "    generator.add(layers.LeakyReLU(alpha=choosen_params['alpha']))\n",
    "    if i != choosen_params['conv2d_layers'] - 1:\n",
    "      generator.add(layers.BatchNormalization(momentum=choosen_params['momentum']))\n",
    "  generator.add(layers.Conv2DTranspose(1, (int(kernels[-1]), int(kernels[-1])),\n",
    "                                           padding='same', activity_regularizer=keras.regularizers.L2(choosen_params['l2']), strides=(2,2)))\n",
    "  generator.add(layers.LeakyReLU(alpha=choosen_params['alpha']))\n",
    "  return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czKBUGAY7eSI"
   },
   "outputs": [],
   "source": [
    "#Saves Models that train for all the epochs and returns dataframe of model paramters and results\n",
    "def random_search(iter, epochs, param_space):\n",
    "  for i in range(iter):\n",
    "    params_df = pd.DataFrame(columns=['tuner_iter','g_loss', 'd_loss', 'epochs_before_failure','dense','conv2d_layers', \n",
    "                                      'alpha', 'l2', 'filter_size', 'filter_growth','kernel_size', 'kernel_growth' ,'momentum', 'learning_rate', 'steps_per_lr_decay'])\n",
    "    #Gets random params from param space\n",
    "    params = get_params(param_space)\n",
    "\n",
    "    #Builds generator with random parameters\n",
    "    generator = build_generator(iter, params)\n",
    "    \n",
    "    #builds discriminator with same params every time\n",
    "    discriminator = build_discriminator()\n",
    "\n",
    "    #generator learning rate is random, discriminator learning rate is constant\n",
    "    g_initial_learning_rate = params['learning_rate']\n",
    "    d_initial_learning_rate = 0.001\n",
    "    #Learning rate decays at rate initial_learning_rate * .9986^(decay_steps/steps_per_epoch)\n",
    "    #generator learning rate decay steps is random\n",
    "    g_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "      g_initial_learning_rate,\n",
    "      decay_steps=params['steps_per_lr_decay'],\n",
    "      decay_rate=.9986\n",
    "      )\n",
    "  \n",
    "    d_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    d_initial_learning_rate,\n",
    "    decay_steps=steps_per_epoch,\n",
    "    decay_rate=.9986\n",
    "    )\n",
    "    #creates condtional gan\n",
    "    cond_gan = ConditionalGAN(\n",
    "      discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "    \n",
    "    cond_gan.compile(\n",
    "      d_optimizer=keras.optimizers.Adam(learning_rate=d_lr_schedule),\n",
    "      g_optimizer=keras.optimizers.Adam(learning_rate=g_lr_schedule),\n",
    "      loss_fn=keras.losses.BinaryCrossentropy(),\n",
    "    )\n",
    "\n",
    "    custom_checkpoint = Custom_Callback()\n",
    "\n",
    "    history = cond_gan.fit(dataset, epochs=epochs, callbacks=[custom_checkpoint], verbose=2)\n",
    "\n",
    "    #adds params and training results to params_df\n",
    "    params['tuner_iter'] = i\n",
    "    params['g_loss'] = history.history['g_loss'][-1]\n",
    "    params['d_loss'] = history.history['d_loss'][-1]\n",
    "    params['epochs_before_failure'] = len(history.history['d_loss'])\n",
    "    params_df = params_df.append(params, ignore_index=True)\n",
    "    \n",
    "\n",
    "    #if the model trained for the full number of epochs without failing then the model is saved\n",
    "    if len(history.history['g_loss']) == epochs:\n",
    "      generator.save(path_for_folder_to_save_models + '/generator_' + str(i))\n",
    "    \n",
    "    #models are deleted\n",
    "    del generator\n",
    "    del discriminator\n",
    "    del cond_gan\n",
    "    keras.backend.clear_session()\n",
    "  return params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "xnk0FyVWqE1o",
    "outputId": "722f5043-0e14-4635-d565-f416f60968bd"
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-235e4e60da0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossible_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-6317669a8cad>\u001b[0m in \u001b[0;36mrandom_search\u001b[0;34m(iter, epochs, param_space)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#Builds generator with random parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#builds discriminator with same params every time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2b6a7aaa18d7>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m(iter, choosen_params)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoosen_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv2d_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     generator.add(layers.Conv2DTranspose(filters[i], (int(kernels[i]), int(kernels[i])),\n\u001b[0;32m---> 19\u001b[0;31m                                            padding='same', activity_regularizer=keras.regularizers.L2(choosen_params['l2']), strides=strides[i]))\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchoosen_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mchoosen_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv2d_layers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1831\u001b[0m     return tf.random.uniform(\n\u001b[1;32m   1832\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m         seed=self.make_legacy_seed())\n\u001b[0m\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,32,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
     ]
    }
   ],
   "source": [
    "params_df = random_search(search_iterations, epochs, possible_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dN4HDzE-eA3"
   },
   "outputs": [],
   "source": [
    "params_df.to_csv(path_for_folder_to_save_models + '/params_df.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CGAN_random_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
