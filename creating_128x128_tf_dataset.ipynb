{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cearting_128x128_tf_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmt_iC3qKcdj"
      },
      "source": [
        "path_for_joined_geotiff_df = '' \n",
        "path_for_1km_by_1km_folder = ''\n",
        "path_to_save_tf_dataset = ''"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajdaWFE9cEjs",
        "outputId": "25c095d7-720d-4943-d087-73cf9fda8246"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#from google.colab import drive\n",
        "import pandas as pd\n",
        "from osgeo import gdal\n",
        "from skimage.transform import resize\n",
        "#drive.mount('/content/drive')\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvIJjN9K-kUo"
      },
      "source": [
        "df = pd.read_csv(path_for_joined_geotiff_df)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeIV-lo1WhZX"
      },
      "source": [
        "#These Rows (from yoesmetie) had random points with elevation in the billions, so I just exluded them\n",
        "outliers = [964, 970, 975, 976, 977, 982, 983, 984, 985, 992, 993, 994, 995]\n",
        "df = df[~df.index.isin(outliers)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2joAuWp5veXH"
      },
      "source": [
        "image_size = 128\n",
        "batch_size = 32\n",
        "num_channels = 1\n",
        "splits = 1000//image_size"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ou0X5TxsXGP"
      },
      "source": [
        "#Gets file name from the df and reads raster data from 1km_by_1km file\n",
        "file_head = path_for_1km_by_1km_folder\n",
        "dataset = []\n",
        "labels = []\n",
        "for index, row in df.iterrows():\n",
        "  path =  file_head + '/' +str(row.file_name)\n",
        "  ds = gdal.Open(path)\n",
        "  ds = np.array(ds.GetRasterBand(1).ReadAsArray())\n",
        "  ds[ds < 0] = 0\n",
        "  #Some tiles are .5 meter resolution, size 2000*2000, and some are 2 meter resolution, size 500*500.\n",
        "  if np.shape(ds) != (1000, 1000):\n",
        "    #resizes all tiles to 1000*1000\n",
        "    ds = resize(ds, (1000, 1000))\n",
        "  for i in range(splits):\n",
        "    ds_split = ds[image_size*i:image_size*(i+1), image_size*i:image_size*(i+1)]\n",
        "    dataset.append(ds_split)\n",
        "    labels.append(row.BIOME_NAME)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxf8aYrFdTaw"
      },
      "source": [
        "steps_per_epoch = int(np.ceil(len(dataset)/batch_size))\n",
        "num_classes = len(np.unique(labels))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HiX0_wiJFQ8"
      },
      "source": [
        "def normalize(dataset):\n",
        "  return 2*(dataset - np.min(dataset))/(np.max(dataset) - np.min(dataset))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhXBKUix0AUM"
      },
      "source": [
        "dataset = normalize(dataset)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x-1AcJubTOH"
      },
      "source": [
        "#Adds channel to data\n",
        "dataset = np.expand_dims(dataset, axis=-1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDhRS30dZwjM"
      },
      "source": [
        "#Class Labels\n",
        "labels = pd.factorize(labels)\n",
        "all_labels = keras.utils.to_categorical(labels[0], num_classes)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjLNhoEcbWjJ"
      },
      "source": [
        "#Creates tesnor of raster data with class labels\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dataset, all_labels))\n",
        "dataset = dataset.shuffle(buffer_size=len(dataset)).batch(batch_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiMAyKrgG9Tm"
      },
      "source": [
        "tf.data.experimental.save(\n",
        "    dataset, path_for_folder_to_save_data, compression=None, shard_func=None, checkpoint_args=None)"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}