{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmt_iC3qKcdj"
   },
   "outputs": [],
   "source": [
    "path_for_joined_geotiff_df = '' \n",
    "path_for_folder_to_save_models = ''\n",
    "#Path for tensorflow 128x128 dataset\n",
    "path_for_tf_dataset = ''\n",
    "#epochs to train gan for\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajdaWFE9cEjs",
    "outputId": "be6bc535-65cb-4d28-e15a-1b880baf9318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from google.colab import drive\n",
    "import pandas as pd\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCSA0LKF-PjS"
   },
   "outputs": [],
   "source": [
    "#Parameters from generator_18 from the random search\n",
    "generator_18_params = {'alpha': 0.8,\n",
    " 'conv2d_layers': 3,\n",
    " 'dense': True,\n",
    " 'filter_growth': 'decreasing',\n",
    " 'filter_size': 4,\n",
    " 'kernel_growth': 'increasing',\n",
    " 'kernel_size': 4,\n",
    " 'l2': \t0.000001,\n",
    " 'learning_rate': 0.01,\n",
    " 'momentum': 0.3,\n",
    " 'steps_per_lr_decay': 35}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvIJjN9K-kUo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_for_joined_geotiff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeIV-lo1WhZX"
   },
   "outputs": [],
   "source": [
    "#These Rows (from yoesmetie) had random points with elevation in the billions, so I just exluded them\n",
    "outliers = [964, 970, 975, 976, 977, 982, 983, 984, 985, 992, 993, 994, 995]\n",
    "df = df[~df.index.isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2joAuWp5veXH"
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ou0X5TxsXGP"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.experimental.load(path_for_tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxf8aYrFdTaw"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(len(dataset)/batch_size))\n",
    "num_classes = len(np.unique(df.BIOME_NAME))\n",
    "latent_dim = ((16*16)*num_classes)-num_classes\n",
    "generator_input = latent_dim + num_classes\n",
    "discriminator_in_channels = 1 + num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgfaqFXHIJiw"
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "  discriminator = keras.Sequential(\n",
    "      [\n",
    "          keras.layers.InputLayer((image_size, image_size, discriminator_in_channels)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\" \n",
    "                        , activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\" \n",
    "                        , activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.Conv2D(256, (4, 4), strides=(2, 2), padding=\"same\" \n",
    "                        , activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.Conv2D(512, (4, 4), strides=(2, 2), padding=\"same\" ,\n",
    "                        activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.BatchNormalization(momentum=0.5),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.Conv2D(1024, (3, 3), strides=(2, 2), padding=\"same\", \n",
    "                         activity_regularizer=keras.regularizers.L2(.00001)),\n",
    "          layers.LeakyReLU(alpha=0.2),\n",
    "          layers.GlobalMaxPooling2D(),\n",
    "          layers.Dense(1, activation='sigmoid')\n",
    "      ],\n",
    "      name=\"discriminator\",\n",
    "  )\n",
    "  return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQmPkPlhISfz"
   },
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        #load data\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dimension for the labels\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        )\n",
    "\n",
    "        # Generate random points in the latent space.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        #Add the labels\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "        #Generate fake terrain with generator\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        #Combine terrain with labels\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        #Real and Fake Terrain\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "        \n",
    "        #Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        #Track loss\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "\n",
    "        return{\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoCavfVztcVP"
   },
   "outputs": [],
   "source": [
    "#Returns 2 lists of kernel and filters sizes for each conv2d_layer\n",
    "def get_kernels_and_filters(first_filter_size, filter_growth, first_kernel_size, kernel_growth, conv2d_layers):\n",
    "  #Used to constrain the size of the model when increasing and decreasing growth for kernel and filter size\n",
    "  max_filter = 2048\n",
    "  min_filter = 4\n",
    "  max_kernel = 32\n",
    "  min_kernel = 4\n",
    "  #Creates list of filter sizes for each conv2D layer depending on if the filters are increasing, constant, or decreasing in size\n",
    "  if filter_growth == 'constant':\n",
    "    filters = [first_filter_size for i in range(conv2d_layers)]\n",
    "  elif filter_growth == 'increasing':\n",
    "    filters = [first_filter_size*2**i for i in range(conv2d_layers)]\n",
    "  else:\n",
    "    filters = [first_filter_size*2**-i for i in range(conv2d_layers)]\n",
    "  #Creates list of kernel sizes for each conv2D layer depending on if the kernels are increasing, constant, or decreasing in size\n",
    "  if kernel_growth == 'constant':\n",
    "    kernels = [first_kernel_size for i in range(conv2d_layers)]\n",
    "  elif kernel_growth == 'increasing':\n",
    "    kernels = [first_kernel_size*2**i for i in range(conv2d_layers)]\n",
    "  else:\n",
    "    kernels = [first_kernel_size*2**-i for i in range(conv2d_layers)]\n",
    "  kernels =  np.asarray(kernels)\n",
    "  filters = np.asarray(filters)\n",
    "  #if filter or kernel size is greater/less than the max/min filter or kernel size then it is set to the max/min\n",
    "  kernels[kernels < min_kernel] = min_kernel\n",
    "  kernels[kernels > max_kernel] = max_kernel\n",
    "  filters[filters > max_filter] = max_filter\n",
    "  filters[filters < min_filter] = min_filter\n",
    "  return kernels, filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkM28DQlxf8j"
   },
   "outputs": [],
   "source": [
    "#returns list of strides for each conv2d layer\n",
    "def get_strides(conv2d_layers):\n",
    "  #Every models last two conv2d layers will have (2,2) strides\n",
    "  strides = [(2,2), (2,2)]\n",
    "  for i in range(conv2d_layers-3):\n",
    "    strides.insert(0,(1,1))\n",
    "  return strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye2CfRcWQqy3"
   },
   "outputs": [],
   "source": [
    "class Custom_Callback(keras.callbacks.Callback):\n",
    "   def on_epoch_end(self, epoch, logs=None):\n",
    "    #Saves generator and discriminator every 20 epochs\n",
    "    if (epoch%20 == 0) and (epoch != 0):\n",
    "       cond_gan.generator.save(path_for_folder_to_save_models + '/generator_18_' + str(epoch))\n",
    "       cond_gan.discriminator.save(path_for_folder_to_save_models + '/discriminator_' + str(epoch))\n",
    "    #Stops training if discriminator loss or generator loss gets close to zero\n",
    "    if (logs.get('d_loss') < .0001) or (logs.get('g_loss') < .0001):\n",
    "         self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOB0Ua3nhcQY"
   },
   "outputs": [],
   "source": [
    "#Builds generator with the random params\n",
    "def build_generator(choosen_params):\n",
    "  strides = get_strides(choosen_params['conv2d_layers'])\n",
    "  #Gets lists of kernel sizes, and filter sizes\n",
    "  kernels, filters = get_kernels_and_filters(first_filter_size=choosen_params['filter_size'], filter_growth=choosen_params['filter_growth'], \n",
    "                          first_kernel_size=choosen_params['kernel_size'], kernel_growth=choosen_params['kernel_growth'], conv2d_layers=choosen_params['conv2d_layers'])\n",
    "  generator = keras.Sequential()\n",
    "  generator.add(keras.layers.InputLayer(generator_input,))\n",
    "  #Maps latent space to 2d space using dense layers\n",
    "  if choosen_params['dense']:\n",
    "    generator.add(layers.Dense(16*16*num_classes, activity_regularizer=keras.regularizers.L2(choosen_params['l2'])))\n",
    "    generator.add(layers.LeakyReLU(alpha=choosen_params['alpha']))\n",
    "    generator.add(layers.BatchNormalization(momentum=choosen_params['momentum']))\n",
    "    generator.add(layers.Reshape((16, 16, num_classes)))\n",
    "  else:\n",
    "    #If dense layer isn't used then latent space is reshaped to 2d\n",
    "    generator.add(layers.Reshape((16, 16, num_classes)))\n",
    "  for i in range(choosen_params['conv2d_layers']-1):\n",
    "    generator.add(layers.Conv2DTranspose(filters[i], (int(kernels[i]), int(kernels[i])),\n",
    "                                           padding='same', activity_regularizer=keras.regularizers.L2(choosen_params['l2']), strides=strides[i]))\n",
    "    generator.add(layers.LeakyReLU(alpha=choosen_params['alpha']))\n",
    "    #If last layer batch normalization is not used\n",
    "    if i != choosen_params['conv2d_layers'] - 1:\n",
    "      generator.add(layers.BatchNormalization(momentum=choosen_params['momentum']))\n",
    "  generator.add(layers.Conv2DTranspose(1, (int(kernels[-1]), int(kernels[-1])),\n",
    "                                           padding='same', activity_regularizer=keras.regularizers.L2(choosen_params['l2']), strides=(2,2)))\n",
    "  generator.add(layers.LeakyReLU(alpha=choosen_params['alpha']))\n",
    "  return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjPITEGEoj_k"
   },
   "outputs": [],
   "source": [
    "#Builds generator with random parameters\n",
    "generator_18 = build_generator(generator_18_params)\n",
    "    \n",
    "  #builds discriminator with same params every time\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSPYi6Slolde"
   },
   "outputs": [],
   "source": [
    "  #generator learning rate is random, discriminator learning rate is constant\n",
    "g_initial_learning_rate = generator_18_params['learning_rate']\n",
    "d_initial_learning_rate = 0.001\n",
    "\n",
    "\n",
    "#Learning rate decays at rate initial_learning_rate * .9986^(decay_steps/steps_per_epoch)\n",
    "#generator learning rate decay\n",
    "g_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "g_initial_learning_rate,\n",
    "decay_steps=generator_18_params['steps_per_lr_decay'],\n",
    "decay_rate=.9986\n",
    ")\n",
    "  \n",
    "d_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "d_initial_learning_rate,\n",
    "decay_steps=steps_per_epoch,\n",
    "decay_rate=.9986\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czKBUGAY7eSI"
   },
   "outputs": [],
   "source": [
    "#creates condtional gan\n",
    "cond_gan = ConditionalGAN(\n",
    "  discriminator=discriminator, generator=generator_18, latent_dim=latent_dim)\n",
    "#Compiles GAN    \n",
    "cond_gan.compile(\n",
    "  d_optimizer=keras.optimizers.Adam(learning_rate=d_lr_schedule),\n",
    "  g_optimizer=keras.optimizers.Adam(learning_rate=g_lr_schedule),\n",
    "  loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "#Creates the callback\n",
    "custom_checkpoint = Custom_Callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5A6KPE3zopkG"
   },
   "outputs": [],
   "source": [
    "history = cond_gan.fit(dataset, epochs=epochs, callbacks=[custom_checkpoint], verbose=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "generator_18_from_random_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
